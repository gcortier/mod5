# Mod 3 Exposer une base de donnÃ©es relationnelle via une API REST et entrainement d'un modÃ¨le
###### Le `.venv` 


```bash
python -m venv .venv
```


* **Windows (PowerShell) :**
    ```bash
    .\.venv\Scripts\Activate.ps1
    ```
* **Windows (CMD) :**
    ```bash
    .\.venv\Scripts\activate.bat
    ```
* **macOS / Linux :**
    ```bash
    source .venv/bin/activate
    ```


###### Le `requirements.txt`


Assure-toi que ton `.venv` est activÃ©, puis :

```bash
pip install -r requirements.txt
```


#### ğŸ—ºï¸ Architecture : OÃ¹ va quoi dans notre petit monde ? ğŸ—ºï¸


.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data-all-684bf775c031b265646213.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ models.py
â”‚   â”œâ”€â”€ model_2024_08.pkl
â”‚   â””â”€â”€ preprocessor.pkl
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ alchemy-api.log
â”‚   â””â”€â”€ main_api.log
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â””â”€â”€ print_draw.py
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ main_api.py => API FastAPI 
â”œâ”€â”€ alchemy_api.py => Niveau 0 API
â””â”€â”€ requirements.txt
```

###### `data/`
* `data-all-complete-684bf9cd92797851623245.csv` : les donnÃ©es du dernier cas avec des colonnes en plus

###### `figures/`
Sauvegarde des images des courbes de coÃ»t et autres graphiques pour visualiser les performances de notre modÃ¨le.

###### `models/`
* `models.py` : La dÃ©finition du model
* `model_20250620_1536_0_None.pkl` :Dernier model entrainÃ©
* `preprocessor_latest.pkl` : Le preprocess avec les nouvelles colonnnes !

###### `modules/`
Ce sont nos couteaux suisses du code. Chaque fichier est un expert dans son domaine.
* `evaluate.py` : Le juge impitoyable qui dit si notre modÃ¨le est un gÃ©nie ou un cancre.
* `preprocess.py` : Le chef cuisinier des donnÃ©es. Il les nettoie, les coupe, les assaisonne pour qu'elles soient parfaites pour notre IA.
* `print_draw.py` : L'artiste du groupe. Il transforme nos chiffres barbares en beaux graphiques pour que mÃªme ta grand-mÃ¨re puisse comprendre (enfin, presque).

###### `notebooks/`
les diffÃ©rents notebook pour explorer les briefs


###### `simplonsql/`
Le model de donnÃ©es utilisÃ© pour la base de donnÃ©es SQLAlchemy et les routes FastApi

---

# TD => GOGOGO
## setup

- ### GÃ©nÃ©ration requirements.txt Ã  chaque installation de module
```bash
pip freeze > requirements.txt
```

- ### Installations des requis sqlAlchemy: 
```bash
pip install psycopg2-binary
pip install pydantic-sqlalchemy
```
- ### Installations des requis Mod3 Brief 2: 
```bash
pip install tensorflow
```

- ### Installations des requis loguru: 
```bash
pip install loguru
```

- ### Installations des requis FastAPI/Streamlit: 
```bash
pip install nltk fastapi streamlit uvicorn requests pydantic
```
- #### Pour lancer le serveur MLflow :
```bash
uvicorn main_api:app --host 127.0.0.1 --port 8000 --reload
```
- #### Description des routes de l'API FastAPI :
[GET /docs](http://127.0.0.1:8000/docs#/)


- ### Installation des bibliothÃ¨ques pour les tests unitaires: 
```bash
pip install pytest httpx
pytest test_predict_api.py
```

- ### Installations des requis pour MLflow : 
  > **mlFlow**
  MlFlow est un outil de gestion des expÃ©riences de machine learning. Il permet de suivre les expÃ©riences, de gÃ©rer les modÃ¨les et de visualiser les rÃ©sultats.
```bash
pip install mlflow scikit-learn pandas matplotlib
```

# Pour lancer le serveur MLflow :
```bash
mlflow ui
```

## Streamlit
lancer le serveur Streamlit pour l'interface utilisateur :
```bash
streamlit run streamlit_app.py
```
### Pour accÃ©der au front (root + pages entrainnement et prediction :)
[Streamlit Front](http://localhost:8501)



- FastAPI : http://localhost:8000/docs
- MLflow UI : http://localhost:5000


## setup docker postgresql
```powershell
docker compose -f docker-compose-postgres.yml up -d
```

## Etape clÃ©s du projet
- J'ai crÃ©Ã© un nouveau projet Ã  partir des modules prÃ©cÃ©dent : FastAPI / MLFlow / Streamlit/ LOGURU / Docker / 
- Le jeu de donnÃ©es est stockÃ© dans le dossier `data/` sous le nom `data-all-684bf775c031b265646213.csv`.
- J'ai suivi le module sur la prise en main de SQL Alchimy et la gestion ORM
- Le notebook est disponible dans le dossier `notebooks/` sous le nom `SQLAlchemy_exploration.ipynb`.
- Utilisation du notebook crÃ©Ã© lors du module 2 pour analyser et crÃ©er un jeu de donnÃ©es propre (CSV).
- RÃ©utilisation du notebook crÃ©Ã© lors du module 2 pour analyser le jeu de donnÃ©es, comprendre les donnÃ©es et les nettoyer.
- Le notebook est disponible dans le dossier `notebooks/` sous le nom `ethique_data_cleaning.ipynb`.
- J'ai crÃ©Ã© un script `python alchemy_api.py clean_dataset` pour nettoyer le jeu de donnÃ©es et le stocker dans le dossier `data/`.
- J'ai crÃ©Ã© une DB PostgreSQL containerisÃ© que j'ai rempli avec les valeurs  j'ai crÃ©Ã© un CRUD sur la ressource Client. J'ai Ã©galement crÃ©Ã© un prÃ©processeur et j'ai crÃ©Ã© un modÃ¨le que j'ai entraÃ®nÃ© avec les donnÃ©es prÃ©processÃ©es de la DB.
- J'ai changÃ© le preprocesseur pour s'adapter aux nouvelles colonnes du jeu de donnÃ©es.
- J'ai crÃ©Ã© des scripts utilitaires pour injecter les donnÃ©es `python inject_data.py init`
- Ajout du module pydantic-sqlalchemy pour gÃ©rer les changements de modÃ¨les et rendre + dynamique pydantic
- Une route `/predict_loandata42` pour faire des prÃ©dictions sur des donnÃ©es envoyÃ©es via pydantic
- le loging des performances avec `loguru` et un setup simplifiÃ© pour le logger.
- les images des couts stockÃ©s dans le dossier `figures/`.


## Quelles difficultÃ©s jâ€™ai rencontrÃ©es ? 
- l'apprentissage des nouvelles librairies et leur implÃ©mentation dans une architecture relativement propre, la rÃ©utilisation d"outils dÃ©ja vu (MLFlow ...). 
- Je suis repassÃ© sur sqllite en db car trop de soucis avec postgres
- La gestion des migrations de la base de donnÃ©es avec Alembic.
- La gestion des donnÃ©es manquantes et la crÃ©ation d'un prÃ©processeur efficace.


## Quâ€™est-ce que jâ€™ai appris ? 
- J'ai appris Ã  utiliser l'ORM SQLAlchemy et Ã  gÃ©rer une DB avec.
- J'ai consolidÃ© les outils Ã  ma disposition pour crÃ©er une API REST et un modÃ¨le de machine learning : 
- TransfÃ©rer des poids d'un modÃ¨le Ã  un autre.
- Comment utiliser des stratÃ©gy de remplissage de colonne en gardans l'information originale : missing True/False
  - Simplification de l'initialisation 
  ```python 
    ## Base initialisation for Loguru and FastAPI
    from myapp_base import setup_loguru, app, Request, HTTPException
    logger = setup_loguru("logs/alchemy_api.log")
  ```





> **Note :** OK mes qustionnements on Ã©tÃ© rÃ©pondu lors de la session en one to one

### Flux de donnÃ©es et transformations

- Les donnÃ©es brutes sont importÃ©es depuis un fichier CSV (`data-all-complete-684bf9cd92797851623245.csv`).
- Un nettoyage Ã©thique est rÃ©alisÃ© dans le notebook `ethique_data_cleaning_complete.ipynb`Â :  
  - Suppression des colonnes sensibles ou non conformes (nom, prÃ©nom, sexe, nationalitÃ©, orientation sexuelle, date de crÃ©ation de compte).
  - Remplissage des valeurs manquantes pour certaines colonnes (`loyer_mensuel` par la moyenne, `situation_familiale` par la modalitÃ© la plus frÃ©quente).
  - Filtrage des valeurs aberrantes (poids, nb_enfants, quotient_caf, loyer_mensuel).
  - Suppression des colonnes avec trop de valeurs manquantes (`score_credit`, `historique_credits`).
- Le dataset nettoyÃ© est sauvegardÃ© sous `df_data_all_complete_cleaned.csv` puis injectÃ© dans la base via le script `inject_data.py`.

### SchÃ©ma de la base de donnÃ©es

- Table principaleÂ : `loandatas`
- ColonnesÂ :  
  `id`, `age`, `taille`, `poids`, `nb_enfants`, `quotient_caf`, `sport_licence`, `niveau_etude`, `region`, `smoker`, `revenu_estime_mois`, `situation_familiale`, `risque_personnel`, `loyer_mensuel`, `montant_pret`
- Le schÃ©ma est versionnÃ© et documentÃ© via les migrations Alembic.

### Ã‰valuation Ã©thique

- Les colonnes Ã  risque de discrimination ou dâ€™identification ont Ã©tÃ© supprimÃ©es.
- Les choix de remplissage et de filtrage sont documentÃ©s dans le notebook.
- Les risques de biais restants sont identifiÃ©s 

---


# Brief 2 :
Objectif : Adapter un modÃ¨le IA existant (TensorFlow/Keras) Ã  de nouvelles colonnes dâ€™entrÃ©e, sans perdre les poids internes, puis le rÃ©entraÃ®ner sur un dataset enrichi.
Suivi des performancesâ€¯: MLflow ou TensorBoard.
Exposition du modÃ¨leâ€¯: API FastAPI prenant en compte les nouvelles colonnes.
Livrables attendusâ€¯:
Nouveau modÃ¨le entraÃ®nÃ© (.h5, .pkl ou SavedModel)
Script de modification de lâ€™architecture + script dâ€™entraÃ®nement
Script/projet FastAPI pour lâ€™API
Journal de suivi MLflow
README dÃ©taillÃ©


- j'ai refacto le code pour rendre dynamique les usages de colonnes pour l'entrainnement du model
- J'ai ajoutÃ© la stratÃ©gie de missing pour les colonnes incompletes (Remplissage + indication dans une colonne dÃ©diÃ© pour savoir si la valeur etait manquante ou pas Ã  la base)
- entraÃ®nement du model : python alchemy_api.py train
  
## Performance 1er entrainnement :
==================Performance for run 0/1===================
MSE: 77444596.8351, MAE: 6463.7837, RÂ²: 0.3169
============================================================


- test avec notebook
- Codage Codage Codage...
- Ajout stratÃ©gie de transfert de poids en option de settings


## ExÃ©cution du script d'entraÃ®nement en transferant les poids du premier model
60/60 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 1ms/step 
==================Performance for run 0/1===================
MSE: 61944948.3360, MAE: 5717.0758, RÂ²: 0.4537
============================================================
Model enregistrÃ© dans models/model_20250620_1536_0_None.pkl
Courbe de loss : model_20250620_1536_0_None.jpg



- CrÃ©ation d'un script pour enregistrer le preprocess pour la route de prÃ©diction
- Changement de type pour la donnÃ©es 

- Utilisation d'une data d'enregistrement de l'ordre des colonnes pour completer la route de prÃ©diction.